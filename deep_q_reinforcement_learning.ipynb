{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-q-reinforcement-learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7wS/PoW+nBwjsrYKjOPmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatchFramework/deep-q-reinforcement-learning/blob/main/deep_q_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q Reinforcement Learning \n"
      ],
      "metadata": {
        "id": "S1ijfZOeOy7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to uncomment the following line to install all dependencies, if you are not using google colab and you have not installed them already:"
      ],
      "metadata": {
        "id": "2K7L3AMbP5G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch matplotlib pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKEpRxmbQEaU",
        "outputId": "263111cb-4f23-49e4-ce98-1f1e56f1330e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ikSvUeeZOnAt"
      },
      "outputs": [],
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple, deque\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Creating the Network Policy"
      ],
      "metadata": {
        "id": "nxYVyy1RQ1WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQReinforcementPolicy(nn.Module):\n",
        "  def __init__(self, lr, in_dims, l1_dims, l2_dims, out_dims):\n",
        "    \"\"\"\n",
        "    This class contains the neural network policy that is used for estimating the q values for each action that the agent can make.\n",
        "    The q values are an estimate of the potential reward that the agent can expect for an action.\n",
        "\n",
        "    Parameters:\n",
        "    ---\n",
        "    lr: learning rate during training\n",
        "    in_dims: Dimensions of the input; this is equal to the size of the state vector\n",
        "    l1_dims: The amount of neuron connections that the first fully connected layer of the policy should have\n",
        "    l2_dims: The amount of neuron connections that the second fully connected layer has\n",
        "    out_dims: Output dimensions of the network; equal to the amount of actions, that the agent can perform in the environment; they will return the q-value for each action\n",
        "    \"\"\"\n",
        "    # Initilize the class and the input parameters\n",
        "    super(DeepQReinforcementPolicy, self).__init__()\n",
        "    self.lr = lr\n",
        "    self.in_dims = in_dims\n",
        "    self.l1_dims = l1_dims\n",
        "    self.l2_dims = l2_dims\n",
        "    self.out_dims = out_dims\n",
        "\n",
        "    # define the fully connected layers (fully connected = nn.Linear())\n",
        "    self.l1 = nn.Linear(self.input_dims, self.l1_dims)\n",
        "    self.l2 = nn.Linear(self.l1_dims, self.l2_dims)\n",
        "    self.l3 = nn.Linear(self.l2_dims, self.out_dims)\n",
        "\n",
        "    # the messiah is chosen as the optimizer ;)\n",
        "    self.optim = optim.Adam(self.parameters(), lr=self.lr)\n",
        "    # Mean squared error loss function\n",
        "    self.loss = nn.MSELoss()\n",
        "    \n",
        "    # use the GPU if it is available\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else \"cpu\")\n",
        "    self.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    \"\"\"\n",
        "    Defines how the data is propagated through the layer of the nework.\n",
        "    \"\"\"\n",
        "    x = F.relu(self.l1(state))\n",
        "    x = F.relu(self.l2(x))\n",
        "    action_q_values = self.l3(x)\n",
        "    return action_q_values"
      ],
      "metadata": {
        "id": "Myud5-REQ6_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Creating a Replay Memory\n",
        "\n",
        "The replay memory saves a few past experiences of the Policy. \n",
        "These experiences are past environment states, actions, the resulting state based on that action and the reward that the agent got for that action.\n",
        "The agent is able to use random past experiences instead of consecutive experiences. \n",
        "\n",
        "The concept of a replay memory is that the agent can remember what consequences his actions had in the past. Hence, bad actions that the agend made a long while ago will still have an impact on the agents decision in the present. Therefore, repeating bad decisions can be avoided.\n",
        "\n",
        "In contrast, if no replay memory is used, the agent only remembers the consequences of his last few action. \n",
        "\n",
        "For further documentation see [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
      ],
      "metadata": {
        "id": "AgdXTws8nyag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the Transition object maps the relation between the state and a previous action to the resulting state and the reward for that action\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "  def __init__(self, memory_capacity):\n",
        "    self.memory = deque([],maxlen=memory_capacity)\n",
        "\n",
        "  def push(self, *args):\n",
        "    \"\"\"Save a tuple that records a action and it's consequences.\"\"\"\n",
        "    self.memory.append(Transition(*args))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    \"\"\"Select a random batch of samples from the memory.\"\"\"\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "f5kKDx59pVsv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create the Agent\n",
        "\n",
        "The agent is the piece of code that will take actions in an environment and use the policy to estimate how \"good\" future actions will be (meaning the q-value of the actions). The higher the q-value of an action, the more probable it is, that it will lead to a high reward for the agent.\n",
        "\n",
        "The agent wants to collect as much reward as possible inside the environment."
      ],
      "metadata": {
        "id": "07HJjM5Rt32Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, lr, in_dims, gamma, epsilon, n_actions, batch_size, memory_capacity=100000, epsilon_decrement=0.0001, epsilon_bottom=0.005):\n",
        "    self.lr = lr\n",
        "    self.in_dims = in_dims\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decrement = epsilon_decrement\n",
        "    self.epsilon_bottom = epsilon_bottom\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.action_space = list(range(n_actions))\n",
        "\n",
        "    # stores the last free memory\n",
        "    self.memory_counter = 0\n",
        "\n",
        "    # init the policy\n",
        "    self.eval_q_values = DeepQReinforcementPolicy(lr=self.lr, in_dims=self.in_dims,l1_dims=64, l2_dims=32, out_dims=self.n_actions)\n",
        "\n",
        "    # init the ReplayMemory to sample recent memories from"
      ],
      "metadata": {
        "id": "cbWZBCXhukr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TGs8B0RmO0MN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}